{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Recurrent Neural Network Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Unit\n",
    "Let's define the following recurrent unit\n",
    "$$ h_t = f(W_{hh} h_{t-1} + W_{hx}x_t) $$\n",
    "where $$t=1,\\ldots,\\infty,$$\n",
    "$$h_t\\in\\mathbb{R}^{n\\times 1} \\text{ is the current hidden state}, $$\n",
    "$$h_{t-1}\\in\\mathbb{R}^{n\\times 1} \\text{ is the previous hidden state}, $$\n",
    "$$x_t \\in\\mathbb{R}^{d\\times 1} \\text{ is the current input}, $$\n",
    "$$W_{hh}\\in\\mathbb{R}^{n\\times n}, W_{hx} \\in\\mathbb{R}^{n\\times d} \\text{ are parameter matrices}.$$\n",
    "Here, f(y) is the sigmoid function, $$f(y)=\\frac{1}{1+\\exp(-y)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the sigmoid function look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot sigmoid function in 1D\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(y):\n",
    "    return 1 / (1 + np.exp(-y))\n",
    "\n",
    "# You can change the bound from 5, 10, 20 up to 50 to see the shape zoomed in/out\n",
    "bound = 5\n",
    "x = np.arange(-bound, bound, 0.5)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How would you write matrix operations in code?\n",
    "\n",
    "## Matrix-vector multiplication\n",
    "## A is a n-by-d maxtrix, b is a dx1 vector\n",
    "## c is a n-by-1 vector, the result of multiplying A and b\n",
    "n = 3\n",
    "d = 5\n",
    "A = np.ones([n, d])   # initialize A as a n-by-d all-one matrix\n",
    "print('The size of matrix A is {}-by-{}'.format(A.shape[0], A.shape[1]))\n",
    "b = np.ones([d, 1]) # initialize b as a d-by-1 all-one vector\n",
    "print('The size of vector b is {}-by-{}'.format(b.shape[0], b.shape[1]))\n",
    "c = np.dot(A, b)\n",
    "print('The size of vector c is {}-by-{}'.format(c.shape[0], c.shape[1]))\n",
    "print('The vector c is \\n', c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Function\n",
    "Let's code up the forward function of the recurrent unit.\n",
    "Please fill in `< input your code here >` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given input x, previous state hprev, parameter matrices Whx and Whh\n",
    "## Compute the current state h\n",
    "## Code up the forward function of the recurrent unit define above\n",
    "## Remember to return the current state h\n",
    "\n",
    "hidden_size = 10\n",
    "input_size = 5\n",
    "\n",
    "Whh = np.zeros([hidden_size, hidden_size])\n",
    "Whh += np.random.uniform(-0.1, 0.1, [hidden_size, hidden_size])\n",
    "\n",
    "Whx = np.zeros([hidden_size, input_size])\n",
    "Whx += np.random.uniform(-0.1, 0.1, [hidden_size, input_size])\n",
    "\n",
    "def forward_function(x, hprev, Whx, Whh):\n",
    "    ## first compute the matrix-vector multiplication\n",
    "    # Whx x + Whh hprev\n",
    "    \n",
    "    # TODO:\n",
    "    h = <input your code here>\n",
    "\n",
    "    ## use the sigmoid function to compute the current state\n",
    "    # TODO\n",
    "    h = sigmoid(<input your code here>)\n",
    "    return h\n",
    "\n",
    "\n",
    "x = np.random.randn(input_size, 1)\n",
    "hprev = np.random.randn(hidden_size, 1)\n",
    "h = forward_function(x, hprev, Whx, Whh)\n",
    "\n",
    "print('Successfully forwarded!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test whether the forward function is correct\n",
    "x_test = np.zeros([input_size, 1])\n",
    "hprev_test = np.zeros([hidden_size, 1])\n",
    "h_test = forward_function(x_test, hprev_test, Whx, Whh)\n",
    "assert np.all(h_test - 0.5 < 1e-7)\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Function\n",
    "An important component of finding the best parameters is to train the model by optimization. Let's consider a small, specific example: $ y = 2w^2 + 3$. \n",
    "\n",
    "If we want to *minimize* the value of $y$, what should our weight value $w$ be? We take the derivative of y with respect to w and solve for the value that makes this derivative 0:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial w} = \\frac{\\partial (2w^2 + 3)}{\\partial w} = 4w $$\n",
    "\n",
    "Therefore, the $w$ that minimizes $y$ is $w=0$.\n",
    "\n",
    "For neural networks, usually $y$ is some *cost* function that depends on your *parameter* $w$. Our objective is to find the $w$ that minimizes such cost function. In the recurrent neural network example we're looking at, the parameters we wish to find to minimize $y$ are the weight matrices $W_{hh}$ and $W_{hx}$. Since these are matrices rather than scalars, we need to do a bit of matrix calculus, but the key idea for optimization is the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the important steps:\n",
    "\n",
    "First let us define\n",
    "$$ y = W_{hh} h_{t-1} + W_{hx} x_t.$$\n",
    "Then\n",
    "$$ h_t = f(y) $$\n",
    "#### Important gradient \\#1: \n",
    "\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = W_{hh}^T f\\prime(y),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f\\prime(y)=\\frac{\\partial f(y)}{\\partial y}=\\frac{\\partial}{\\partial y} [\\frac{1}{1+\\exp(-y)}]=\\frac{\\exp(-y)}{(1+\\exp(-y))^2} = (1-f(y))f(y),\n",
    "$$ is the gradient of sigmoid function.\n",
    "\n",
    "\n",
    "Can you verfiy the above gradient? Why would you want to write the gradient in the last form?\n",
    "\n",
    "#### Important gradient \\#2:\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial W_{hh}} = \\frac{\\partial h_t}{\\partial y} h_{t-1}^T = f\\prime(y) h_{t-1}^T\n",
    "$$\n",
    "\n",
    "#### Important gradient \\#3:\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial W_{hx}} = \\frac{\\partial h_t}{\\partial y} x_{t}^T = f\\prime(y) x_{t}^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up a backward function which accepts a training/error signal to weight the gradients and output the three gradients above. \n",
    "Please fill in `< input your code here >` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given a training/error signal dEdh, input x, previous state hprev\n",
    "## parameters Whh, Whx\n",
    "## Return the three gradients above\n",
    "\n",
    "def backward_function(x, hprev, dEdh, Whx, Whh):\n",
    "    ## compute the gradient of sigmoid function\n",
    "    f_prime = <input your code here>\n",
    "    ## weight the gradient by training/error signal\n",
    "    f_prime *= dEdh\n",
    "    ## compute the gradient one \n",
    "    dEdhprev = <input your code here>\n",
    "    ## compute the gradient two\n",
    "    dWhh = <input your code here>\n",
    "    ## compute the gradient three\n",
    "    dWhx = <input your code here>\n",
    "    return f_prime.T, dWhx, dWhh\n",
    "\n",
    "# compute gradients\n",
    "E = np.sum(h)\n",
    "dEdh = np.ones([hidden_size, 1])\n",
    "\n",
    "dEdhprev, dWhx, dWhh = backward_function(x, hprev, dEdh, Whx, Whh)\n",
    "\n",
    "print('Successfully backpropgated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check whether your code is right\n",
    "\n",
    "# Numerical gradient computation\n",
    "epsilon = 1e-7                                         \n",
    "numdWhh = np.zeros([hidden_size,hidden_size])       \n",
    "for i in range(hidden_size):                           \n",
    "    for j in range(hidden_size):                       \n",
    "        newWhh = np.copy(Whh)                          \n",
    "        newWhh[i,j] += epsilon                         \n",
    "                                                           \n",
    "        h = forward_function(x, hprev, Whx, newWhh)\n",
    "        newE = np.sum(h)                               \n",
    "        numdWhh[i,j] = (newE - E) / epsilon            \n",
    "                                                           \n",
    "diff = np.sum(numdWhh - dWhh)   \n",
    "print('Diff is ', diff)assert diff < 1e-3                                     \n",
    "print('Test Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have learned the key part of a recurrent neural network.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-venv",
   "language": "python",
   "name": "python3-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
