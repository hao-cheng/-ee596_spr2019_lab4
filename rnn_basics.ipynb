{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Recurrent Neural Network Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Unit\n",
    "Let's define the following recurrent unit\n",
    "$$ h_t = f(W_{hh} h_{t-1} + W_{hx}x_t) $$\n",
    "where $$t=1,\\ldots,\\infty,$$\n",
    "$$h_t\\in\\mathbb{R}^{n\\times 1} \\text{ is the current hidden state}, $$\n",
    "$$h_{t-1}\\in\\mathbb{R}^{n\\times 1} \\text{ is the previous hidden state}, $$\n",
    "$$x_t \\in\\mathbb{R}^{d\\times 1} \\text{ is the current input}, $$\n",
    "$$W_{hh}\\in\\mathbb{R}^{n\\times n}, W_{hx} \\in\\mathbb{R}^{n\\times d} \\text{ are parameter matrices}.$$\n",
    "Here, f(y) is the sigmoid function, $$f(y)=\\frac{1}{1+\\exp(-y)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the sigmoid function look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot sigmoid function in 1D\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(y):\n",
    "    return 1 / (1 + np.exp(-y))\n",
    "\n",
    "# You can change the bound from 5, 10, 20 up to 50 to see the shape zoomed in/out\n",
    "bound = 5\n",
    "x = np.arange(-bound, bound, 0.5)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## How would you write matrix operations in code?\n",
    "\n",
    "## Matrix-vector multiplication\n",
    "## A is a n-by-d maxtrix, b is a dx1 vector\n",
    "## c is a n-by-1 vector, the result of multiplying A and b\n",
    "n = 3\n",
    "d = 5\n",
    "A = np.ones([n, d])   # initialize A as a n-by-d all-one matrix\n",
    "print('The size of matrix A is {}-by-{}'.format(A.shape[0], A.shape[1]))\n",
    "b = np.ones([d, 1]) # initialize b as a d-by-1 all-one vector\n",
    "print('The size of vector b is {}-by-{}'.format(b.shape[0], b.shape[1]))\n",
    "c = np.dot(A, b)\n",
    "print('The size of vector c is {}-by-{}'.format(c.shape[0], c.shape[1]))\n",
    "print('The vector c is \\n', c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Function\n",
    "Let's code up the forward function of the recurrent unit.\n",
    "Please fill in `< input your code here >` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given input x, previous state hprev, parameter matrices Whx and Whh\n",
    "## Compute the current state h\n",
    "## Code up the forward function of the recurrent unit define above\n",
    "## Remember to return the current state h\n",
    "\n",
    "hidden_size = 10\n",
    "input_size = 5\n",
    "\n",
    "Whh = np.zeros([hidden_size, hidden_size])\n",
    "Whh += np.random.uniform(-0.1, 0.1, [hidden_size, hidden_size])\n",
    "\n",
    "Whx = np.zeros([hidden_size, input_size])\n",
    "Whx += np.random.uniform(-0.1, 0.1, [hidden_size, input_size])\n",
    "\n",
    "def forward_function(x, hprev, Whx, Whh):\n",
    "    ## first compute the matrix-vector multiplication\n",
    "    # Whx x + Whh hprev\n",
    "    \n",
    "    # TODO:\n",
    "    h = <input your code here>\n",
    "\n",
    "    ## use the sigmoid function to compute the current state\n",
    "    # TODO\n",
    "    h = sigmoid(<input your code here>)\n",
    "    return h\n",
    "\n",
    "\n",
    "x = np.random.randn(input_size, 1)\n",
    "hprev = np.random.randn(hidden_size, 1)\n",
    "h = forward_function(x, hprev, Whx, Whh)\n",
    "\n",
    "print('Successfully forwarded!')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test whether the forward function is correct\n",
    "x_test = np.zeros([input_size, 1])\n",
    "hprev_test = np.zeros([hidden_size, 1])\n",
    "h_test = forward_function(x_test, hprev_test, Whx, Whh)\n",
    "assert np.all(h_test - 0.5 < 1e-7)\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Function\n",
    "An important component of finding the best parameters is to train the model by optimization. Let's consider a small, specific example: $ E(w) = 2w^2 + 3$. \n",
    "\n",
    "If we want to *minimize* the value of $E$, what should our weight value $w$ be? We take the derivative of $E(w)$ with respect to w and solve for the value that makes this derivative 0:\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial w} = \\frac{\\partial (2w^2 + 3)}{\\partial w} = 4w $$\n",
    "\n",
    "Therefore, the $w$ that minimizes $E$ is $w=0$.\n",
    "\n",
    "In this toy example, we get a closed-form solution for the minimal value of $E$, but this might not be possible for all functions. In neural networks, usually the solution is found iteratively, i.e. we adjust the weights by nudging them in the direction that helps optimize $E$. \n",
    "\n",
    "For neural networks, usually $E$ is some *cost* (error) function that depends on your *parameter* $w$. Our objective is to find the $w$ that minimizes such cost function. In the recurrent neural network example we're looking at, the parameters we wish to find to minimize $E$ are the weight matrices $W_{hh}$ and $W_{hx}$. Since these are matrices rather than scalars, we need to do a bit of matrix calculus, but the key idea for optimization is the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the important steps:\n",
    "\n",
    "First let us define\n",
    "$$ y = W_{hh} h_{t-1} + W_{hx} x_t.$$\n",
    "Then\n",
    "$$ h_t = f(y) = sigmoid(y) $$\n",
    "#### Important gradient \\#1: \n",
    "\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = W_{hh}^T f'(y),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f'(y)=\\frac{\\partial f(y)}{\\partial y}=\\frac{\\partial}{\\partial y} [\\frac{1}{1+\\exp(-y)}]=\\frac{\\exp(-y)}{(1+\\exp(-y))^2} = (1-f(y))f(y),\n",
    "$$ is the gradient of sigmoid function.\n",
    "\n",
    "\n",
    "Can you verfiy the above gradient? Why would you want to write the gradient in the last form?\n",
    "\n",
    "#### Important gradient \\#2:\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial W_{hh}} = \\frac{\\partial h_t}{\\partial y} h_{t-1}^T = f'(y) h_{t-1}^T\n",
    "$$\n",
    "\n",
    "#### Important gradient \\#3:\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial W_{hx}} = \\frac{\\partial h_t}{\\partial y} x_{t}^T = f'(y) x_{t}^T\n",
    "$$\n",
    "\n",
    "\n",
    "#### Making use of the error signal:\n",
    "The unit we just considered usually makes up *one* layer of a neural network. Let us consider an error signal, i.e. a signal that quantifies how far away we are from an optimal solution. This signal is often a function of the hidden unit: \n",
    "$$loss_t = E(h_t)$$\n",
    "\n",
    "Ultimately, we want to use this information to nudge the parameters in our network in the right direction and therefore need to compute $ \\frac{\\partial E(h_t)}{\\partial y} $. By the chain rule:\n",
    "$$ \\frac{\\partial E(h_t)}{\\partial y} = \\frac{\\partial E(h_t)}{\\partial h_t} \\frac{\\partial h_t}{\\partial y} = \\text{dEdh} f'(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up a backward function which accepts a training/error signal to weight the gradients and output the three gradients above. \n",
    "Please fill in `< input your code here >` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given a training/error signal dEdh, input x, previous state hprev\n",
    "## parameters Whh, Whx\n",
    "## Return the three gradients above\n",
    "\n",
    "def backward_function(x, hprev, dEdh, Whx, Whh):\n",
    "    ## compute the gradient of sigmoid function\n",
    "    f_prime = <input your code here>\n",
    "    ## weigh the gradient by training/error signal\n",
    "    f_prime *= dEdh\n",
    "    ## compute gradient #1 \n",
    "    dEdhprev = <input your code here>\n",
    "    ## compute gradient #2\n",
    "    dWhh = <input your code here>\n",
    "    ## compute gradient #3\n",
    "    dWhx = <input your code here>\n",
    "    return dEdhprev, dWhx, dWhh\n",
    "\n",
    "# compute gradients\n",
    "h = forward_function(x, hprev, Whx, Whh)\n",
    "E = np.sum(h)\n",
    "dEdh = np.ones([hidden_size, 1])\n",
    "\n",
    "dEdhprev, dWhx, dWhh = backward_function(x, hprev, dEdh, Whx, Whh)\n",
    "\n",
    "print('Successfully backpropgated!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check whether your code is right\n",
    "\n",
    "# Numerical gradient computation\n",
    "epsilon = 1e-7                                         \n",
    "numdWhh = np.zeros([hidden_size, hidden_size])       \n",
    "for i in range(hidden_size):                           \n",
    "    for j in range(hidden_size):                       \n",
    "        newWhh = np.copy(Whh)                          \n",
    "        newWhh[i,j] += epsilon                         \n",
    "                                                           \n",
    "        h = forward_function(x, hprev, Whx, newWhh)\n",
    "        newE = np.sum(h)                               \n",
    "        numdWhh[i,j] = (newE - E) / epsilon            \n",
    "                                                           \n",
    "diff = abs(np.sum(numdWhh - dWhh))\n",
    "print('Diff is ', diff)\n",
    "assert diff < 1e-3                                     \n",
    "print('Test Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have learned the key part of a recurrent neural network.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-venv",
   "language": "python",
   "name": "python3-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
